# Reading List

- [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/pdf/1909.08053). September 2019
- [Memorizing Transformers](https://arxiv.org/pdf/2203.08913). March 2022
- [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/pdf/2104.04473). April 2021
- [Scaling Language Models: Methods, Analysis & Insights from Training Gopher](https://arxiv.org/pdf/2112.11446). December 2021
- [LaMDA: Language Models for Dialog Applications](https://arxiv.org/pdf/2201.08239). January 2022
- [Improving language models by retrieving from trillions of tokens](https://arxiv.org/pdf/2112.04426). December 2021
- [[DALL-E] Zero-Shot Text-to-Image Generation](https://arxiv.org/pdf/2102.12092). February 2021
- [[Transformer] Attention Is All You Need](https://arxiv.org/pdf/1706.03762). June 2017
- [Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361). January 2020
- [[CLIP] Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020). February 2021
- [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692). July 2019
- [Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model](https://arxiv.org/pdf/2201.11990). January 2022
- [Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers](https://arxiv.org/pdf/2109.10686). September 2021
- [[Chinchilla] Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556). March 2022