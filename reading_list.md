# Reading List

- [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/pdf/1909.08053). September 2019
- [Memorizing Transformers](https://arxiv.org/pdf/2203.08913). March 2022
- [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/pdf/2104.04473). April 2021
- [Scaling Language Models: Methods, Analysis & Insights from Training Gopher](https://arxiv.org/pdf/2112.11446). December 2021
- [LaMDA: Language Models for Dialog Applications](https://arxiv.org/pdf/2201.08239). January 2022
- [Improving language models by retrieving from trillions of tokens](https://arxiv.org/pdf/2112.04426). December 2021
- [[DALL-E] Zero-Shot Text-to-Image Generation](https://arxiv.org/pdf/2102.12092). February 2021
- [[Transformer] Attention Is All You Need](https://arxiv.org/pdf/1706.03762). June 2017
- [Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361). January 2020
- [[CLIP] Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020). February 2021
- [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692). July 2019
- [Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model](https://arxiv.org/pdf/2201.11990). January 2022
- [Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers](https://arxiv.org/pdf/2109.10686). September 2021
- [[Chinchilla] Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556). March 2022
- [A data-driven approach for learning to control computers](https://arxiv.org/pdf/2202.08137). February 2022. DeepMind
- [Data Distributional Properties Drive Emergent Few-Shot Learning in Transformers](https://arxiv.org/pdf/2205.05055). April 2022. Stanford, University College London, DeepMind
- [CvT: Introducing Convolutions to Vision Transformers](https://arxiv.org/pdf/2103.15808). March 2021. Microsoft, Microsoft Cloud + AI, McGill University
- [[DETR] End-to-End Object Detection with Transformers](https://arxiv.org/pdf/2005.12872). May 2020. Facebook
- [[Vision Transformer] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/pdf/2010.11929). October 2020. Google Research, Google, Google Brain
- [It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners](https://arxiv.org/pdf/2009.07118). September 2020. Sulzer, LMU Munich
- [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/pdf/2101.03961). January 2021. Google
- [Pathways: Asynchronous Distributed Dataflow for ML](https://arxiv.org/pdf/2203.12533). March 2022. Google
- [GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/pdf/2006.16668). June 2020. Google
- [Flamingo: a Visual Language Model for Few-Shot Learning](https://arxiv.org/pdf/2204.14198). April 2022. DeepMind
- [Carbon Emissions and Large Neural Network Training](https://arxiv.org/pdf/2104.10350). April 2021. Google, Berkeley.
- [[Meena] Towards a Human-like Open-Domain Chatbot](https://arxiv.org/pdf/2001.09977). January 2020. Google, Google Brain, Google Research
- [The Evolved Transformer](https://arxiv.org/pdf/1901.11117). January 2019. Google, Google Research, Google Brain
- [Decision Transformer: Reinforcement Learning via Sequence Modeling](https://arxiv.org/pdf/2106.01345). June 2021. Google, Facebook, UC Berkeley, Google Brain
- [Perceiver IO: A General Architecture for Structured Inputs & Outputs](https://arxiv.org/pdf/2107.14795). July 2021. DeepMind
- [An Evolutionary Approach to Dynamic Introduction of Tasks in Large-scale Multitask Learning Systems](https://arxiv.org/pdf/2205.12755). May 2022. Google Research, Google
- [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/pdf/2205.14135). May 2022. University at Buffalo, Stanford
- [Linformer: Self-Attention with Linear Complexity](https://arxiv.org/pdf/2006.04768). June 2020. Facebook
- [A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/pdf/2002.05709). February 2020. Google Brain, Google Research, Google
- [Momentum Contrast for Unsupervised Visual Representation Learning](https://arxiv.org/pdf/1911.05722). November 2019. Facebook
- [Deep Double Descent: Where Bigger Models and More Data Hurt](https://arxiv.org/pdf/1912.02292). December 2019. OpenAI, Harvard
- [Extracting Training Data from Large Language Models](https://arxiv.org/pdf/2012.07805). December 2020. Northeastern, Harvard, UC Berkeley, Google, Apple, OpenAI, Stanford
- [An Empirical Model of Large-Batch Training](https://arxiv.org/pdf/1812.06162). December 2018. OpenAI, Johns Hopkins. [Blog](https://openai.com/blog/science-of-ai/)
- [LayoutLM: Pre-training of Text and Layout for Document Image Understanding](https://arxiv.org/pdf/1912.13318). December 2019. 
- [Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models](https://arxiv.org/pdf/2102.02503). February 2021. Stanford, OpenAI
- [Pretrained Transformers as Universal Computation Engines](https://arxiv.org/pdf/2103.05247). March 2021. Facebook, Google Brain, UC Berkeley, Google
- [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/pdf/2204.02311). April 2022. Google, Google Research
- [GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/pdf/2006.16668). June 2020. Google
- [Pathways: Asynchronous Distributed Dataflow for ML](https://arxiv.org/pdf/2203.12533). March 2022. Google
- [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/pdf/2101.03961). January 2021. Google
- [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683). October 2019. Google
- [An Evolutionary Approach to Dynamic Introduction of Tasks in Large-scale Multitask Learning Systems](https://arxiv.org/pdf/2205.12755). May 2022. Google, Google Research