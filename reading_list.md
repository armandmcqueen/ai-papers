# Reading List

- [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/pdf/1909.08053). September 2019
- [Memorizing Transformers](https://arxiv.org/pdf/2203.08913). March 2022
- [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/pdf/2104.04473). April 2021
- [Scaling Language Models: Methods, Analysis & Insights from Training Gopher](https://arxiv.org/pdf/2112.11446). December 2021
- [LaMDA: Language Models for Dialog Applications](https://arxiv.org/pdf/2201.08239). January 2022
- [Improving language models by retrieving from trillions of tokens](https://arxiv.org/pdf/2112.04426). December 2021
- [[DALL-E] Zero-Shot Text-to-Image Generation](https://arxiv.org/pdf/2102.12092). February 2021
- [[Transformer] Attention Is All You Need](https://arxiv.org/pdf/1706.03762). June 2017
- [Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361). January 2020
- [[CLIP] Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/pdf/2103.00020). February 2021
- [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/pdf/1907.11692). July 2019
- [Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model](https://arxiv.org/pdf/2201.11990). January 2022
- [Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers](https://arxiv.org/pdf/2109.10686). September 2021
- [[Chinchilla] Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556). March 2022
- [A data-driven approach for learning to control computers](https://arxiv.org/pdf/2202.08137). February 2022. DeepMind
- [Data Distributional Properties Drive Emergent Few-Shot Learning in Transformers](https://arxiv.org/pdf/2205.05055). April 2022. Stanford, University College London, DeepMind
- [CvT: Introducing Convolutions to Vision Transformers](https://arxiv.org/pdf/2103.15808). March 2021. Microsoft, Microsoft Cloud + AI, McGill University
- [[DETR] End-to-End Object Detection with Transformers](https://arxiv.org/pdf/2005.12872). May 2020. Facebook
- [[Vision Transformer] An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/pdf/2010.11929). October 2020. Google Research, Google, Google Brain
- [It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners](https://arxiv.org/pdf/2009.07118). September 2020. Sulzer, LMU Munich
- [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/pdf/2101.03961). January 2021. Google
- [Pathways: Asynchronous Distributed Dataflow for ML](https://arxiv.org/pdf/2203.12533). March 2022. Google
- [GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/pdf/2006.16668). June 2020. Google